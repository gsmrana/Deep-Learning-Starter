{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85a0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "# disable keras backend for transformers\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"USE_JAX\"] = \"0\"\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff7e0085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4069b94",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36efc446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29fc072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'input', 'instruction', 'text'],\n",
       "        num_rows: 51760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_instruction(example):\n",
    "    return {\n",
    "        \"text\": f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['input']}\\n### Response:\\n{example['output']}\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_instruction)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987beab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n",
       " 'input': '',\n",
       " 'instruction': 'Give three tips for staying healthy.',\n",
       " 'text': '### Instruction:\\nGive three tips for staying healthy.\\n### Input:\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e687300",
   "metadata": {},
   "source": [
    "### Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aec40a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "LlamaForCausalLM                                   --\n",
       "├─LlamaModel: 1-1                                  --\n",
       "│    └─Embedding: 2-1                              262,668,288\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                 --\n",
       "│    │    │    └─LlamaAttention: 4-1               --\n",
       "│    │    │    │    └─Linear4bit: 5-1              (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-2              (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-3              (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-4              (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-2                     --\n",
       "│    │    │    │    └─Linear4bit: 5-5              (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-6              (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-7              (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-8          --\n",
       "│    │    │    └─LlamaRMSNorm: 4-3                 2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-4                 2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-2                 --\n",
       "│    │    │    └─LlamaAttention: 4-5               --\n",
       "│    │    │    │    └─Linear4bit: 5-9              (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-10             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-11             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-12             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-6                     --\n",
       "│    │    │    │    └─Linear4bit: 5-13             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-14             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-15             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-16         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-7                 2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-8                 2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-3                 --\n",
       "│    │    │    └─LlamaAttention: 4-9               --\n",
       "│    │    │    │    └─Linear4bit: 5-17             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-18             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-19             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-20             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-10                    --\n",
       "│    │    │    │    └─Linear4bit: 5-21             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-22             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-23             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-24         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-11                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-12                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-4                 --\n",
       "│    │    │    └─LlamaAttention: 4-13              --\n",
       "│    │    │    │    └─Linear4bit: 5-25             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-26             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-27             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-28             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-14                    --\n",
       "│    │    │    │    └─Linear4bit: 5-29             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-30             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-31             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-32         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-15                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-16                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-5                 --\n",
       "│    │    │    └─LlamaAttention: 4-17              --\n",
       "│    │    │    │    └─Linear4bit: 5-33             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-34             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-35             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-36             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-18                    --\n",
       "│    │    │    │    └─Linear4bit: 5-37             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-38             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-39             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-40         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-19                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-20                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-6                 --\n",
       "│    │    │    └─LlamaAttention: 4-21              --\n",
       "│    │    │    │    └─Linear4bit: 5-41             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-42             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-43             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-44             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-22                    --\n",
       "│    │    │    │    └─Linear4bit: 5-45             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-46             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-47             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-48         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-23                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-24                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-7                 --\n",
       "│    │    │    └─LlamaAttention: 4-25              --\n",
       "│    │    │    │    └─Linear4bit: 5-49             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-50             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-51             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-52             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-26                    --\n",
       "│    │    │    │    └─Linear4bit: 5-53             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-54             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-55             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-56         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-27                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-28                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-8                 --\n",
       "│    │    │    └─LlamaAttention: 4-29              --\n",
       "│    │    │    │    └─Linear4bit: 5-57             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-58             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-59             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-60             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-30                    --\n",
       "│    │    │    │    └─Linear4bit: 5-61             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-62             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-63             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-64         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-31                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-32                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-9                 --\n",
       "│    │    │    └─LlamaAttention: 4-33              --\n",
       "│    │    │    │    └─Linear4bit: 5-65             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-66             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-67             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-68             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-34                    --\n",
       "│    │    │    │    └─Linear4bit: 5-69             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-70             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-71             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-72         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-35                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-36                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-10                --\n",
       "│    │    │    └─LlamaAttention: 4-37              --\n",
       "│    │    │    │    └─Linear4bit: 5-73             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-74             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-75             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-76             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-38                    --\n",
       "│    │    │    │    └─Linear4bit: 5-77             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-78             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-79             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-80         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-39                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-40                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-11                --\n",
       "│    │    │    └─LlamaAttention: 4-41              --\n",
       "│    │    │    │    └─Linear4bit: 5-81             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-82             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-83             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-84             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-42                    --\n",
       "│    │    │    │    └─Linear4bit: 5-85             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-86             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-87             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-88         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-43                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-44                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-12                --\n",
       "│    │    │    └─LlamaAttention: 4-45              --\n",
       "│    │    │    │    └─Linear4bit: 5-89             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-90             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-91             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-92             (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-46                    --\n",
       "│    │    │    │    └─Linear4bit: 5-93             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-94             (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-95             (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-96         --\n",
       "│    │    │    └─LlamaRMSNorm: 4-47                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-48                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-13                --\n",
       "│    │    │    └─LlamaAttention: 4-49              --\n",
       "│    │    │    │    └─Linear4bit: 5-97             (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-98             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-99             (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-100            (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-50                    --\n",
       "│    │    │    │    └─Linear4bit: 5-101            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-102            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-103            (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-104        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-51                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-52                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-14                --\n",
       "│    │    │    └─LlamaAttention: 4-53              --\n",
       "│    │    │    │    └─Linear4bit: 5-105            (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-106            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-107            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-108            (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-54                    --\n",
       "│    │    │    │    └─Linear4bit: 5-109            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-110            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-111            (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-112        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-55                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-56                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-15                --\n",
       "│    │    │    └─LlamaAttention: 4-57              --\n",
       "│    │    │    │    └─Linear4bit: 5-113            (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-114            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-115            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-116            (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-58                    --\n",
       "│    │    │    │    └─Linear4bit: 5-117            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-118            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-119            (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-120        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-59                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-60                2,048\n",
       "│    │    └─LlamaDecoderLayer: 3-16                --\n",
       "│    │    │    └─LlamaAttention: 4-61              --\n",
       "│    │    │    │    └─Linear4bit: 5-121            (2,097,152)\n",
       "│    │    │    │    └─Linear4bit: 5-122            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-123            (524,288)\n",
       "│    │    │    │    └─Linear4bit: 5-124            (2,097,152)\n",
       "│    │    │    └─LlamaMLP: 4-62                    --\n",
       "│    │    │    │    └─Linear4bit: 5-125            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-126            (8,388,608)\n",
       "│    │    │    │    └─Linear4bit: 5-127            (8,388,608)\n",
       "│    │    │    │    └─SiLUActivation: 5-128        --\n",
       "│    │    │    └─LlamaRMSNorm: 4-63                2,048\n",
       "│    │    │    └─LlamaRMSNorm: 4-64                2,048\n",
       "│    └─LlamaRMSNorm: 2-3                           2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                   --\n",
       "├─Linear: 1-2                                      262,668,288\n",
       "===========================================================================\n",
       "Total params: 1,011,943,424\n",
       "Trainable params: 525,404,160\n",
       "Non-trainable params: 486,539,264\n",
       "==========================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,   # Optional for memory efficiency\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    result = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# prepare smaller subset for quick fine-tuning\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "summary(model, depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fbea01",
   "metadata": {},
   "source": [
    "### LoRA Fine-Tuning Setup (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b2bcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "PeftModelForCausalLM                                              --\n",
       "├─LoraModel: 1-1                                                  --\n",
       "│    └─LlamaForCausalLM: 2-1                                      --\n",
       "│    │    └─LlamaModel: 3-1                                       750,979,072\n",
       "│    │    └─Linear: 3-2                                           (262,668,288)\n",
       "==========================================================================================\n",
       "Total params: 1,013,647,360\n",
       "Trainable params: 1,703,936\n",
       "Non-trainable params: 1,011,943,424\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c4791",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95494cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m#compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[32m     38\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# fine tune the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:1850\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1849\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1861\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:375\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    374\u001b[39m     past_seen_tokens = past_key_values.get_seq_length() \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     cache_position: torch.Tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_seen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seen_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    380\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_checkpoints\",\n",
    "    #remove_unused_columns=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    max_steps=200,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    #tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# fine tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231e44c",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e158ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "finetuned_model_path = \"./llama3-finetuned\"\n",
    "\n",
    "model.save_pretrained(finetuned_model_path)\n",
    "tokenizer.save_pretrained(finetuned_model_path)\n",
    "\n",
    "from peft import merge_and_unload\n",
    "\n",
    "model = merge_and_unload(model)\n",
    "model.save_pretrained(f\"{finetuned_model_path}-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15aab5d",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629573d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_pipeline = pipeline(\"text-generation\", model=finetuned_model_path, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "print(finetuned_pipeline(prompt, max_new_tokens=100)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
