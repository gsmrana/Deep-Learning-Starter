{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c31dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# disable keras backend for transformers\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"USE_JAX\"] = \"0\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f88e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f4b81",
   "metadata": {},
   "source": [
    "### Text Classification (product reviews, social media analysis, chatbot tone control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d4c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " [{'label': 'POSITIVE', 'score': 0.9997356534004211}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "text_classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "\n",
    "text = \"I love how easy Hugging Face makes NLP!\"\n",
    "result = text_classifier(text)\n",
    "print(\"\\nResponse:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7d5ab",
   "metadata": {},
   "source": [
    "### Text Generation (chatbots, content generation, creative writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec31348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " In the future, artificial intelligence will be able to build algorithms that can be used to determine the order in which items are sold, the price they charge for them, the quality of the products they sell, and other factors that influence the price of items.\n",
      "\n",
      "But there are a number of issues that must be resolved before we can see how these algorithms can be used to determine the order of items, or for instance, when the sales staff will check out the goods.\n",
      "\n",
      "In this case, the problem is that we are trying to determine the order of the items within our shopping cart, rather than the order of the products within the shopping cart.\n",
      "\n",
      "It\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai-community/gpt2\"\n",
    "generator_pipeline = pipeline(task=\"text-generation\", model=model_name)\n",
    "\n",
    "prompt = \"In the future, artificial intelligence will\"\n",
    "result = generator_pipeline(prompt, max_new_tokens=128, num_return_sequences=1)\n",
    "print(\"\\nResponse:\\n\", result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da7036",
   "metadata": {},
   "source": [
    "### Instruct Model (next word prediction base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49557889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " Learning Hugging Face models can be a bit overwhelming at first, but breaking it down into step-by-step instructions will make it more manageable. Here's a comprehensive guide to get you started:\n",
      "\n",
      "**Step 1: Install the Required Packages**\n",
      "\n",
      "To start using Hugging Face models, you'll need to install the following packages:\n",
      "\n",
      "* `transformers`: This is the main package for Hugging Face models.\n",
      "* `torch`: This is a popular deep learning library for Python.\n",
      "* `torchvision`: This is a library that provides a wide range of pre-trained models and tools for building and fine-tuning models.\n",
      "\n",
      "You can install these packages using pip:\n",
      "```bash\n",
      "pip install transformers torch torchvision\n",
      "```\n",
      "**Step 2: Install a Python Environment**\n",
      "\n",
      "To run the code, you'll need to have a Python environment set up. You can use an IDE like PyCharm, Visual Studio Code, or Spyder, or you can use a Python environment manager like conda.\n",
      "\n",
      "**Step 3: Create a New Project**\n",
      "\n",
      "Create a new directory for your project and navigate to it in your terminal or command prompt. Then, run the following command to create a new Python file:\n",
      "```bash\n",
      "python -m venv myenv\n",
      "```\n",
      "This will create a new virtual environment called `myenv`.\n",
      "\n",
      "**Step 4: Activate the Environment**\n",
      "\n",
      " Activate the environment by running:\n",
      "```bash\n",
      "source myenv/bin/activate\n",
      "```\n",
      "On Windows, you can use the following command:\n",
      "```bash\n",
      "myenv\\Scripts\\activate\n",
      "```\n",
      "**Step 5: Install Transformers and Torch**\n",
      "\n",
      "Install the `transformers` and `torch` packages using pip:\n",
      "```bash\n",
      "pip install transformers torch torchvision\n",
      "```\n",
      "**Step 6: Install a Pre-trained Model**\n",
      "\n",
      "Hugging Face provides a wide range of pre-trained models. You can use the `transformers` package to load and use these models.\n",
      "\n",
      "Here's an example of how to load a pre-trained model:\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "# Load the pre-trained model and tokenizer\n",
      "model_name = \"distilbert-base-uncased\"\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\n",
      "# Use the model and tokenizer to load a pre-trained model\n",
      "text = \"This is an example sentence.\"\n",
      "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
      "```\n",
      "**Step 7: Fine-tune a Model**\n",
      "\n",
      "Fine-tuning a model involves\n"
     ]
    }
   ],
   "source": [
    "# required: huggingface-cli login <access_token>\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "#model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "#model_name = \"google/gemma-3-4b-it\"\n",
    "#model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "#model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "chat_pipeline = pipeline(task=\"text-generation\", model=model_name, dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assitannt\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you tell me how can I learn to use hugging face models step by step?\"}\n",
    "]\n",
    "\n",
    "response = chat_pipeline(chat, max_new_tokens=512)\n",
    "print(\"\\nResponse:\\n\", response[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a40705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The capital of Bangladesh is Dhaka.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HUGGINGFACEHUB_ACCESS_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of Bangladesh?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63dc77",
   "metadata": {},
   "source": [
    "### Text Summarization (news summarization, report compression, academic paper abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf8a5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " Transformers are deep learning models designed to process sequential data . They use self-attention mechanisms to weigh the importance of different words in a sentence . This architecture has become the foundation for\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer_pipeline = pipeline(task=\"summarization\", model=model_name)\n",
    "\n",
    "text = \"\"\"\n",
    "Transformers are deep learning models designed to process sequential data.\n",
    "They use self-attention mechanisms to weigh the importance of different words in a sentence.\n",
    "This architecture has become the foundation for many state-of-the-art NLP systems.\n",
    "\"\"\"\n",
    "summary = summarizer_pipeline(text, max_length=40, min_length=10, do_sample=False)\n",
    "print(\"\\nResponse:\\n\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb205ba",
   "metadata": {},
   "source": [
    "### Question Answering (search systems, customer support bots, document assistants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28572c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " {'score': 0.8040350079536438, 'start': 39, 'end': 44, 'answer': 'tools'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "qa_pipeline = pipeline(task=\"question-answering\", model=model_name)\n",
    "\n",
    "context = \"Hugging Face is a company that creates tools for building applications using machine learning.\"\n",
    "question = \"What does Hugging Face create?\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"\\nResponse:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90ae29",
   "metadata": {},
   "source": [
    "### Named Entity Recognition - NER (resume parsing, information extraction, knowledge graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd5843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "{'entity_group': 'PER', 'score': np.float32(0.99813473), 'word': 'Elon Musk', 'start': 0, 'end': 9}\n",
      "{'entity_group': 'ORG', 'score': np.float32(0.99916387), 'word': 'SpaceX', 'start': 18, 'end': 24}\n",
      "{'entity_group': 'ORG', 'score': np.float32(0.9989968), 'word': 'Twitter', 'start': 44, 'end': 51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nova\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner_pipeline = pipeline(task=\"ner\", model= model_name, grouped_entities=True)\n",
    "\n",
    "text = \"Elon Musk founded SpaceX and later acquired Twitter.\"\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a451f5",
   "metadata": {},
   "source": [
    "### Machine Translation (multilingual chatbots, localization, document translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104192f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      " মেশিন লার্নিং কম্পিউটারকে তথ্য থেকে শিখতে সাহায্য করে।\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"monirbishal/en-bn-nmt\"\n",
    "model_name = \"csebuetnlp/banglat5_nmt_en_bn\"\n",
    "\n",
    "translator_pipeline = pipeline(task=\"translation\", model=model_name)\n",
    "\n",
    "input_text = \"Machine learning enables computers to learn from data.\"\n",
    "result = translator_pipeline(input_text)\n",
    "print(\"\\nResponse:\\n\", result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0710d9",
   "metadata": {},
   "source": [
    "### Text Embedding (semantic search, similarity detection, clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50aa72f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "input_sentences = [\n",
    "    \"Hugging Face makes NLP easy.\",\n",
    "    \"Transformers are powerful for text tasks.\"\n",
    "    ]\n",
    "\n",
    "inputs = tokenizer(input_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    output_embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(\"Embeddings shape:\", output_embeddings.shape)  # e.g. torch.Size([2, 384])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
