{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b61132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "import tokenizers\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "# to get deterministic output\n",
    "#torch.manual_seed(123)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68179a94",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e645ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ZIP_PATH = \"../datasets/fra-eng.zip\"\n",
    "\n",
    "# Download dataset provided by Anki: https://www.manythings.org/anki/ with requests\n",
    "if not os.path.exists(DATASET_ZIP_PATH):\n",
    "    url = \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\"\n",
    "    response = requests.get(url)\n",
    "    with open(DATASET_ZIP_PATH, \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677003ea",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc7e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each line of the file is in the format \"<english>\\t<french>\"\n",
    "# We convert text to lowercasee, normalize unicode (UFKC)\n",
    "def normalize(line):\n",
    "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
    "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
    "    eng, fra = line.split(\"\\t\")\n",
    "    return eng.lower().strip(), fra.lower().strip()\n",
    "\n",
    "text_pairs = []\n",
    "with zipfile.ZipFile(DATASET_ZIP_PATH, \"r\") as zip_ref:\n",
    "    for line in zip_ref.read(\"fra.txt\").decode(\"utf-8\").splitlines():\n",
    "        eng, fra = normalize(line)\n",
    "        text_pairs.append((eng, fra))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f133e",
   "metadata": {},
   "source": [
    "### Tokenization with BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b81a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenization:\n",
      "Original: what's the meaning of this phrase?\n",
      "Tokens: ['Ġwhat', \"'s\", 'Ġthe', 'Ġmeaning', 'Ġof', 'Ġthis', 'Ġphr', 'ase', '?']\n",
      "IDs: [155, 135, 86, 2560, 128, 141, 6183, 301, 26]\n",
      "Decoded:  what's the meaning of this phrase?\n",
      "\n",
      "Original: que veut dire cette phrase ?\n",
      "Tokens: ['[start]', 'Ġque', 'Ġveut', 'Ġdire', 'Ġcette', 'Ġphrase', 'Ġ?', 'Ġ', '[end]']\n",
      "IDs: [0, 116, 778, 374, 291, 2809, 120, 74, 1]\n",
      "Decoded:  que veut dire cette phrase ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_TOKENIZER_EN = \"../datasets/en_tokenizer.json\"\n",
    "DATASET_TOKENIZER_FR = \"../datasets/fr_tokenizer.json\"\n",
    "\n",
    "if os.path.exists(DATASET_TOKENIZER_EN) and os.path.exists(DATASET_TOKENIZER_FR):\n",
    "    en_tokenizer = tokenizers.Tokenizer.from_file(DATASET_TOKENIZER_EN)\n",
    "    fr_tokenizer = tokenizers.Tokenizer.from_file(DATASET_TOKENIZER_FR)\n",
    "else:\n",
    "    en_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "    fr_tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "\n",
    "    # Configure pre-tokenizer to split on whitespace and punctuation, add space at beginning of the sentence\n",
    "    en_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    fr_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "    # Configure decoder: So that word boundary symbol \"Ġ\" will be removed\n",
    "    en_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "    fr_tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "\n",
    "    # Train BPE for English and French using the same trainer\n",
    "    VOCAB_SIZE = 8000\n",
    "    trainer = tokenizers.trainers.BpeTrainer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        special_tokens=[\"[start]\", \"[end]\", \"[pad]\"],\n",
    "        show_progress=True\n",
    "    )\n",
    "    en_tokenizer.train_from_iterator([x[0] for x in text_pairs], trainer=trainer)\n",
    "    fr_tokenizer.train_from_iterator([x[1] for x in text_pairs], trainer=trainer)\n",
    "\n",
    "    en_tokenizer.enable_padding(pad_id=en_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
    "    fr_tokenizer.enable_padding(pad_id=fr_tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
    "\n",
    "    # Save the trained tokenizers\n",
    "    en_tokenizer.save(DATASET_TOKENIZER_EN, pretty=True)\n",
    "    fr_tokenizer.save(DATASET_TOKENIZER_FR, pretty=True)\n",
    "\n",
    "# Test the tokenizer\n",
    "print(\"Sample tokenization:\")\n",
    "en_sample, fr_sample = random.choice(text_pairs)\n",
    "encoded = en_tokenizer.encode(en_sample)\n",
    "print(f\"Original: {en_sample}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print(f\"Decoded: {en_tokenizer.decode(encoded.ids)}\")\n",
    "print()\n",
    "\n",
    "encoded = fr_tokenizer.encode(\"[start] \" + fr_sample + \" [end]\")\n",
    "print(f\"Original: {fr_sample}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")\n",
    "print(f\"Decoded: {fr_tokenizer.decode(encoded.ids)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7519c6",
   "metadata": {},
   "source": [
    "### Define DataLoader for the BPE-encoded translation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341e3dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64accb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: tensor([[  86, 2849,  541,  120,  262,  144, 5999,   75,  410, 2091,  128, 1889,\n",
      "          277, 2421,   12,    2,    2,    2],\n",
      "        [  81,  463,  167,  116,  791,  128, 1325, 1582,  328, 2227,   75, 1564,\n",
      "           12,    2,    2,    2,    2,    2],\n",
      "        [ 140,  105, 1134,  244,  155,   72,  165,   12,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  81,  297, 1050,  159,  698,  135, 1158,   12,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  161,   86,  760, 2939,  201,   81,  157, 1501,   12,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  297,  116,  230,   12,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  235,  305,   81,   75, 1274,   12,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 121, 3444,  318,   75, 2233,  232,  175,  296, 4574,   86, 2154,   12,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 786, 6600, 1004,  318, 1161,  786,   51,   31, 5469,   10,   72,  181,\n",
      "         1161,    2,    2,    2,    2,    2],\n",
      "        [  75, 3064, 1912,   80,  134,  145,   86,  425,   12,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72, 1417,  199,   75, 2971, 1425,   12,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  243,  363,   75,  719, 2170,  150,   81,   12,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 121,  256,  105,  190,  535,   12,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 282,  718,  105,   81,  666,  422,  530,   26,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 821,   81,  251,  391,  150,  159,  309,   12,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 124,  144,   75, 1563, 5401,   12,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [1000,  158,  233, 1597,  186,  894,   75,  809,   12,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  161,   81,   80,  396,  141,  418,   12,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  81,  158,  105,  310,  186,  238,   12,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 112,  230,  223,  585,  474,   72,  158,  824,   81,   26,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 119,  280,   80,  113,  141,   12,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  140,  105,  181,  155,   72,  166,  299,   80,  113,  186,   81,\n",
      "           12,    2,    2,    2,    2,    2],\n",
      "        [ 158,   72,  255,  141,  281,   26,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 121,  896,  125,  405,  105,  320,   12,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  72,  409,  742,  199,  677,  135, 1987,  565,  152,  191,   87,  522,\n",
      "          145,  951,  128, 1270,  442,   12],\n",
      "        [ 165,   81,  255,  694,  493,   26,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  86, 5452,   72, 3113, 1154,   80,  305,  134,   98,  274,   75,  870,\n",
      "         2815, 2152,   12,    2,    2,    2],\n",
      "        [ 944,  572,  551, 5819,  468,  593,  445,  595, 2396,   12,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 328,  119,  177,  985,   10,   72,  140,  105, 1134,  244,   86, 1722,\n",
      "           12,    2,    2,    2,    2,    2],\n",
      "        [ 121,  112,  502, 1437, 1124,   12,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [ 158,   81, 6537,  124,   80,  156,  501,   26,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2],\n",
      "        [  81,  161,  141,   26,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2]])\n",
      "French: tensor([[   0,  129, 2907,  113, 3078,  474, 3354, 6957,  175, 2662, 5514,  103,\n",
      "         1440,  208,  160,  103, 5885,   14,   74,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  157,  125,  523,  117,  517,  848,  103,  211,  228, 2496,  980,\n",
      "          157, 5947,  175, 1704,   14,   74,    1,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  125,  193,  444,  117,  103, 1341,  171, 1016,  103,  130,  116,\n",
      "           93,    8,   84,  235,  219,   74,    1,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  132, 1091, 2461,  127, 1461,  103,  307, 1432,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105,  246,  129,  584, 3541,  116, 1690,  116,  132, 4190,   14,\n",
      "           74,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105,  670,  314,  447,   13, 1627,   14,   74,    1,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105,  551,  193, 2574,   43, 6978,  124, 1682,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  139, 5860,  124, 2738,  123,  712,  158,  102,    8,  164, 1464,\n",
      "           82, 1403,  173, 2197,   14,   74,    1,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0, 1806,   48,  113, 1081,  450, 3430, 1806,   86,  118,   12,  105,\n",
      "          327, 3430,   74,    1,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  124, 5687,   49,  100,    8,   29, 1374,  224,  127, 1120,   14,\n",
      "           74,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105, 2712,   42,   84,  224,  124, 2755,  477, 1905,   14,   74,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,   93,    8,   84,  175, 1068, 2060,  158,  132,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  139,   98,    8, 3003,  117,   89,    8,  740,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  318,   98,    8,   85,   13,  165,  117,  958,  123, 1372,  129,\n",
      "          725,  782,  120,   74,    1,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0, 1161,  404,  158,  307, 1139,   14,   74,    1,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,   91,    8,  461,  175, 2211, 1850,   14,   74,    1,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  429,  125,  408,   92,    8, 2019,  256,  175, 2039,  429,   14,\n",
      "           74,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105,  246,  116,  157, 1699,   85,  130,  618,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  132,  125,  585,  117,  785,  256,  159,   14,   74,    1,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  269,   97,   13,   48,   13,  107,  124,  889, 2436,  161, 1902,\n",
      "          105, 1464,  132, 4286, 2718, 3406,  120,   74,    1,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  159, 1061,  211,  220,   14,   74,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  105,  125,  327,  117,  130,  116,  105,  551,  211,  256,  394,\n",
      "           14,   74,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  495,   13,  352,  469, 1690,   13,  181,  120,   74,    1,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  139,  810,  116,  220,  125,  440,  117, 1541,   14,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,   93,    8, 1204, 1303,  173, 1510,  792,   95,  204,  852,  147,\n",
      "          301,  228,  692,  718,  103,  129, 5380, 3364,  103,  129, 3784, 5407,\n",
      "         5633,  470,   14,   74,    1],\n",
      "        [   0,  333,   13,  187,  552,  234,  116,  130,  512,  743,  120,   74,\n",
      "            1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  127, 5920, 6083,  116,   93,    8,   84, 5243,  100,    8,   29,\n",
      "         1730,  158,  174,  374,  102,    8,  107,   97,  175,   82,  806, 3262,\n",
      "           14,   74,    1,    2,    2],\n",
      "        [   0,  129, 2145,  228,  192, 1194, 3513,  460,  507,  649, 2825,   14,\n",
      "           74,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  980,  159,  416, 1276,   12,  105,  125,  174, 3176,  117,  238,\n",
      "         4375,   14,   74,    1,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  139, 1401,  129, 1706,  144, 6806,  514,   14,   74,    1,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  585,   13,  187,  127, 1075,  396,  606,  306,  120,   74,    1,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2],\n",
      "        [   0,  246,   13,  165,  548,  120,   74,    1,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2]])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, text_pairs):\n",
    "        self.text_pairs = text_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, fra = self.text_pairs[idx]\n",
    "        return eng, \"[start] \" + fra + \" [end]\"\n",
    "\n",
    "def collate_fn(batch):\n",
    "    en_str, fr_str = zip(*batch)\n",
    "    en_enc = en_tokenizer.encode_batch(en_str, add_special_tokens=True)\n",
    "    fr_enc = fr_tokenizer.encode_batch(fr_str, add_special_tokens=True)\n",
    "    en_ids = [enc.ids for enc in en_enc]\n",
    "    fr_ids = [enc.ids for enc in fr_enc]\n",
    "    return torch.tensor(en_ids), torch.tensor(fr_ids)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "dataset = TranslationDataset(text_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test the dataset\n",
    "for en_ids, fr_ids in dataloader:\n",
    "    print(f\"English: {en_ids}\")\n",
    "    print(f\"French: {fr_ids}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee86d4",
   "metadata": {},
   "source": [
    "### Design LSTM seq2seq model for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ded394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"A stacked LSTM encoder with an embedding layer\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Plain LSTM is used. No bidirectional LSTM.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The size of the input vocabulary\n",
    "            embedding_dim: The dimension of the embedding vector\n",
    "            hidden_dim: The dimension of the hidden state\n",
    "            num_layers: The number of recurrent layers (layers of stacked LSTM)\n",
    "            dropout: The dropout rate, applied to all LSTM layers except the last one\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input seq = [batch_size, seq_len] -> embedded = [batch_size, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # outputs = [batch_size, seq_len, embedding_dim]\n",
    "        # hidden = cell = [n_layers, batch_size, hidden_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden, cell):\n",
    "        # input seq = [batch_size, seq_len] -> embedded = [batch_size, seq_len, embedding_dim]\n",
    "        # hidden = cell = [n_layers, batch_size, hidden_dim]\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # output = [batch_size, seq_len, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        \"\"\"Given the partial target sequence, predict the next token\"\"\"\n",
    "        # input seq = [batch_size, seq_len]\n",
    "        # target seq = [batch_size, seq_len]\n",
    "        batch_size, target_len = target_seq.shape\n",
    "        device = target_seq.device\n",
    "        # storing output logits\n",
    "        outputs = []\n",
    "        # encoder forward pass\n",
    "        _enc_out, hidden, cell = self.encoder(input_seq)\n",
    "        dec_in = target_seq[:, :1]\n",
    "        # decoder forward pass\n",
    "        for t in range(target_len-1):\n",
    "            # last target token and hidden states -> next token\n",
    "            pred, hidden, cell = self.decoder(dec_in, hidden, cell)\n",
    "            # store the prediction\n",
    "            pred = pred[:, -1:, :]\n",
    "            outputs.append(pred)\n",
    "            # use the predicted token as the next input\n",
    "            dec_in = torch.cat([dec_in, pred.argmax(dim=2)], dim=1)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6530dc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Seq2SeqLSTM                              --\n",
       "├─EncoderLSTM: 1-1                       --\n",
       "│    └─Embedding: 2-1                    2,048,000\n",
       "│    └─LSTM: 2-2                         1,052,672\n",
       "├─DecoderLSTM: 1-2                       --\n",
       "│    └─Embedding: 2-3                    2,048,000\n",
       "│    └─LSTM: 2-4                         1,052,672\n",
       "│    └─Linear: 2-5                       2,056,000\n",
       "=================================================================\n",
       "Total params: 8,257,344\n",
       "Trainable params: 8,257,344\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameters\n",
    "enc_vocab = len(en_tokenizer.get_vocab())\n",
    "dec_vocab = len(fr_tokenizer.get_vocab())\n",
    "emb_dim = 256\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "# init model\n",
    "encoder = EncoderLSTM(enc_vocab, emb_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "decoder = DecoderLSTM(dec_vocab, emb_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "model = Seq2SeqLSTM(encoder, decoder).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0308ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with:\n",
      "  Input vocabulary size: 8000\n",
      "  Output vocabulary size: 8000\n",
      "  Embedding dimension: 256\n",
      "  Hidden dimension: 256\n",
      "  Number of layers: 2\n",
      "  Dropout: 0.1\n",
      "  Total parameters: 8257344\n"
     ]
    }
   ],
   "source": [
    "print(\"Model created with:\")\n",
    "print(f\"  Input vocabulary size: {enc_vocab}\")\n",
    "print(f\"  Output vocabulary size: {dec_vocab}\")\n",
    "print(f\"  Embedding dimension: {emb_dim}\")\n",
    "print(f\"  Hidden dimension: {hidden_dim}\")\n",
    "print(f\"  Number of layers: {num_layers}\")\n",
    "print(f\"  Dropout: {dropout}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393be749",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace5dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5223/5223 [55:42<00:00,  1.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5; Avg loss 4.2010774445812205; Latest loss 3.4846739768981934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5223/5223 [57:38<00:00,  1.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5; Avg loss 3.3816945481158847; Latest loss 2.665708541870117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5223/5223 [54:23<00:00,  1.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5; Avg loss 2.9767239248983115; Latest loss 2.609898567199707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5223/5223 [41:16<00:00,  2.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5; Avg loss 2.7105143109249834; Latest loss 2.484867572784424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5223/5223 [17:23<00:00,  5.01it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5; Avg loss 2.519286102269907; Latest loss 2.939636468887329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5223/5223 [05:40<00:00, 15.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 2.292828548258117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_WEIGHT_PATH = \"weights/translator.pth\"\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_WEIGHT_PATH), exist_ok=True)\n",
    "\n",
    "if os.path.exists(MODEL_WEIGHT_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
    "else:\n",
    "    N_EPOCHS = 5 # Ideal 30    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=fr_tokenizer.token_to_id(\"[pad]\"))    \n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for en_ids, fr_ids in tqdm.tqdm(dataloader, desc=\"Training\"):\n",
    "            # Move the \"sentences\" to device\n",
    "            en_ids = en_ids.to(device)\n",
    "            fr_ids = fr_ids.to(device)\n",
    "            # zero the grad, then forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(en_ids, fr_ids)\n",
    "            # compute the loss: compare 3D logits to 2D targets\n",
    "            loss = loss_fn(outputs.reshape(-1, dec_vocab), fr_ids[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}; Avg loss {epoch_loss/len(dataloader)}; Latest loss {loss.item()}\")\n",
    "        weigths_name, weigths_ext = os.path.splitext(MODEL_WEIGHT_PATH)\n",
    "        torch.save(model.state_dict(), f\"{weigths_name}-epoch-{epoch+1:02d}{weigths_ext}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        if (epoch+1) % 5 != 0:\n",
    "            continue\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for en_ids, fr_ids in tqdm.tqdm(dataloader, desc=\"Evaluating\"):\n",
    "                en_ids = en_ids.to(device)\n",
    "                fr_ids = fr_ids.to(device)\n",
    "                outputs = model(en_ids, fr_ids)\n",
    "                loss = loss_fn(outputs.reshape(-1, dec_vocab), fr_ids[:, 1:].reshape(-1))\n",
    "                epoch_loss += loss.item()\n",
    "        print(f\"Eval loss: {epoch_loss/len(dataloader)}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e649b",
   "metadata": {},
   "source": [
    "### Test for a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af40a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: her slurred speech was an indication that she was drunk.\n",
      "French: son langage inarticulé trahissait qu'elle était saoule.\n",
      "Predicted:  son tentative a était une était que que quelleelleelle..  \n",
      "\n",
      "English: he has a book.\n",
      "French: il dispose d'un livre.\n",
      "Predicted:  il' une livre. \n",
      "\n",
      "English: i remember when we used to never eat vegetables that we didn't grow ourselves.\n",
      "French: je me rappelle quand nous ne mangions jamais de légumes que nous n'avions pas cultivés nous-mêmes.\n",
      "Predicted:  je me rappelle quand nous ne' jamais jamais''' nous nous nous nous nous nous nous nous..   \n",
      "\n",
      "English: you must have seen them there.\n",
      "French: tu as dû les voir là-bas.\n",
      "Predicted:  tu as probablement voir voir.. \n",
      "\n",
      "English: i know what can happen here.\n",
      "French: je sais ce qu'il peut se passer ici.\n",
      "Predicted:  je sais ce qu' se passe ici. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 5\n",
    "MAX_LEN = 60\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    start_token = torch.tensor([fr_tokenizer.token_to_id(\"[start]\")]).to(device)\n",
    "    for en, true_fr in random.sample(text_pairs, N_SAMPLES):\n",
    "        en_ids = torch.tensor(en_tokenizer.encode(en).ids).unsqueeze(0).to(device)\n",
    "        _output, hidden, cell = model.encoder(en_ids)\n",
    "        pred_ids = [start_token]\n",
    "        for _ in range(MAX_LEN):\n",
    "            decoder_input = torch.tensor(pred_ids).unsqueeze(0).to(device)\n",
    "            output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "            output = output[:, -1, :].argmax(dim=1)\n",
    "            pred_ids.append(output.item())\n",
    "            # early stop if the predicted token is the end token\n",
    "            if pred_ids[-1] == fr_tokenizer.token_to_id(\"[end]\"):\n",
    "                break\n",
    "        # Decode the predicted IDs\n",
    "        pred_fr = fr_tokenizer.decode(pred_ids)\n",
    "        print(f\"English: {en}\")\n",
    "        print(f\"French: {true_fr}\")\n",
    "        print(f\"Predicted: {pred_fr}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
