{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ae7dunqczT8Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# to get deterministic output\n",
        "torch.manual_seed(123)\n",
        "\n",
        "sys.path.append(os.path.abspath(\"..\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "document = \"\"\n",
        "with open(\"../datasets/word_prediction_dataset.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    document = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMU_RwfbzXt4",
        "outputId": "7fe012b9-4425-46b0-f109-e3174731f277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Nova\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Nova\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t28bgAcszaHl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Tokens: 1018\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['about',\n",
              " 'the',\n",
              " 'program',\n",
              " 'what',\n",
              " 'is',\n",
              " 'the',\n",
              " 'course',\n",
              " 'fee',\n",
              " 'for',\n",
              " 'data',\n",
              " 'science',\n",
              " 'mentorship',\n",
              " 'program',\n",
              " '(',\n",
              " 'dsmp',\n",
              " '2023',\n",
              " ')',\n",
              " 'the',\n",
              " 'course',\n",
              " 'follows']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tokens = word_tokenize(document.lower())\n",
        "print(\"Total Tokens:\", len(all_tokens))\n",
        "all_tokens[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G30GxEjgzcfY",
        "outputId": "63c6bf46-c4de-4126-8c12-5f574f70a545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab length: 289\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('<unk>', 0),\n",
              " ('about', 1),\n",
              " ('the', 2),\n",
              " ('program', 3),\n",
              " ('what', 4),\n",
              " ('is', 5),\n",
              " ('course', 6),\n",
              " ('fee', 7),\n",
              " ('for', 8),\n",
              " ('data', 9),\n",
              " ('science', 10),\n",
              " ('mentorship', 11),\n",
              " ('(', 12),\n",
              " ('dsmp', 13),\n",
              " ('2023', 14),\n",
              " (')', 15),\n",
              " ('follows', 16),\n",
              " ('a', 17),\n",
              " ('monthly', 18),\n",
              " ('subscription', 19)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {'<unk>': 0}\n",
        "\n",
        "for token in Counter(all_tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token] = len(vocab)\n",
        "\n",
        "print(\"Vocab length:\", len(vocab))\n",
        "list(vocab.items())[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Text to Numerical Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x52A3E1K1zjn"
      },
      "outputs": [],
      "source": [
        "def text_to_indices(sentence, vocab):\n",
        "  numerical_sentence = []\n",
        "  for token in sentence:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<unk>'])\n",
        "  return numerical_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eu66Zo3e1Wh9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence count: 78\n",
            "Numerical sequence count: 78\n"
          ]
        }
      ],
      "source": [
        "input_sentences = document.split('\\n')\n",
        "numerical_sequences = []\n",
        "\n",
        "for sentence in input_sentences:\n",
        "  tokens = word_tokenize(sentence.lower())\n",
        "  numerical_sequences.append(text_to_indices(tokens, vocab))\n",
        "\n",
        "print(\"Sentence count:\", len(input_sentences))\n",
        "print(\"Numerical sequence count:\", len(numerical_sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Training Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "80rIx4aq6ele"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sequence count: 942\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[1, 2],\n",
              " [1, 2, 3],\n",
              " [4, 5],\n",
              " [4, 5, 2],\n",
              " [4, 5, 2, 6],\n",
              " [4, 5, 2, 6, 7],\n",
              " [4, 5, 2, 6, 7, 8],\n",
              " [4, 5, 2, 6, 7, 8, 9],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3, 12],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3, 12, 13],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3, 12, 13, 14],\n",
              " [4, 5, 2, 6, 7, 8, 9, 10, 11, 3, 12, 13, 14, 15],\n",
              " [2, 6],\n",
              " [2, 6, 16],\n",
              " [2, 6, 16, 17],\n",
              " [2, 6, 16, 17, 18],\n",
              " [2, 6, 16, 17, 18, 19]]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_sequences = []\n",
        "for sequence in numerical_sequences:\n",
        "  for i in range(1, len(sequence)):\n",
        "    training_sequences.append(sequence[:i+1])\n",
        "    \n",
        "print(\"Training sequence count:\", len(training_sequences))\n",
        "training_sequences[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Padding Training Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Z_fiVZ8GRo",
        "outputId": "dc0971ee-ecb8-4061-c6aa-c5d751bb1da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max sequence length: 62\n",
            "Padded Training Sequence length: 62\n"
          ]
        }
      ],
      "source": [
        "seq_lengths = []\n",
        "for sequence in training_sequences:\n",
        "  seq_lengths.append(len(sequence))\n",
        "\n",
        "max_seq_length = max(seq_lengths)\n",
        "print(\"Max sequence length:\", max_seq_length)\n",
        "\n",
        "padded_training_sequence = []\n",
        "for sequence in training_sequences:\n",
        "  padding_length = max_seq_length - len(sequence)\n",
        "  padded_training_sequence.append([0]*padding_length + sequence)\n",
        "  \n",
        "print(\"Padded Training Sequence length:\", len(padded_training_sequence[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split the Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0_wVpepb9iE4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Training Sequence shape: torch.Size([942, 62])\n",
            "X shape: torch.Size([942, 61])\n",
            "y shape: torch.Size([942])\n"
          ]
        }
      ],
      "source": [
        "padded_training_sequence = torch.tensor(padded_training_sequence, dtype=torch.long)\n",
        "print(\"Padded Training Sequence shape:\", padded_training_sequence.shape)\n",
        "\n",
        "X = padded_training_sequence[:, :-1]\n",
        "y = padded_training_sequence[:,-1]\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ed_PLHJ-Dgv",
        "outputId": "7095458b-a2e8-4dc7-f150-80fd56d2c281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   0,   1],\n",
              "        [  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   0,   0,   4],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ...,   0, 285, 176],\n",
              "        [  0,   0,   0,  ..., 285, 176, 286],\n",
              "        [  0,   0,   0,  ..., 176, 286, 287]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eReVrcX9-EUU",
        "outputId": "f2cca2e3-da31-4ef5-bc13-593203462048"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 2,  3,  5,  2,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fR059hVd-IAf"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X.to(device)\n",
        "    self.y = y.to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KLX0clQM_j9r"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(X, y)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the DataLoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch: 0, torch.Size([32, 61]), torch.Size([32])\n",
            "\t tensor([[  0,   0,   0,  ...,   0,  22,  65],\n",
            "        [  0,   0,   0,  ...,  78, 187, 135],\n",
            "        [  0,   0,   0,  ...,  94, 241,  45],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ...,   1,   2, 125],\n",
            "        [  0,   0,   0,  ...,  19, 176, 223],\n",
            "        [  0,   0,   0,  ..., 175,  30,  68]], device='cuda:0') tensor([ 66,  86, 163,  27,  36,   8, 258,   2,   2,  23,   2,  87,  89,  93,\n",
            "        202,  16, 262,   7, 154,   2, 212, 176,  25, 151,  57,  78, 155,  17,\n",
            "          5, 102, 186,   5], device='cuda:0')\n",
            "batch: 1, torch.Size([32, 61]), torch.Size([32])\n",
            "\t tensor([[  0,   0,   0,  ...,  17, 242, 176],\n",
            "        [  0,   0,   0,  ...,  81, 267, 252],\n",
            "        [  0,   0,   0,  ...,  22,  23, 131],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ...,  27,   2,   6],\n",
            "        [  0,   0,   0,  ...,   0,  22,  23],\n",
            "        [  0,   0,   0,  ..., 201,   2, 149]], device='cuda:0') tensor([242, 268, 164, 127,  38, 190,   5, 263, 146,  22,  23,  35, 246,  97,\n",
            "        260,  73,  78,  22,  22, 194,  93,  94, 176,   6, 146,  23,   2, 109,\n",
            "         88,  69,  24,  30], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "  print(f\"batch: {batch_idx}, {batch_x.shape}, {batch_y.shape}\")\n",
        "  print(\"\\t\", batch_x, batch_y)\n",
        "  if batch_idx >= 1:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Design the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0TEukXmWDEn8"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=100)\n",
        "    #self.rnn = nn.RNN(100, 150, batch_first=True)\n",
        "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
        "    #self.gru = nn.GRU(100, 150, batch_first=True)\n",
        "    self.fc = nn.Linear(150, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    #internal_states, hidden_state = self.rnn(embedded)\n",
        "    internal_states, (hidden_state, cell_state) = self.lstm(embedded)\n",
        "    #internal_states, hidden_state = self.gru(embedded)\n",
        "    logits = self.fc(hidden_state.squeeze(0))\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================\n",
            "Layer (type:depth-idx)                   Kernel Shape     Input Shape      Output Shape     Param #\n",
            "========================================================================================================\n",
            "SimpleLSTM                               --               [32, 61]         [32, 289]        --\n",
            "├─Embedding: 1-1                         --               [32, 61]         [32, 61, 100]    28,900\n",
            "│    └─weight                            [100, 289]                                         └─28,900\n",
            "├─LSTM: 1-2                              --               [32, 61, 100]    [32, 61, 150]    151,200\n",
            "│    └─weight_ih_l0                      [600, 100]                                         ├─60,000\n",
            "│    └─weight_hh_l0                      [600, 150]                                         ├─90,000\n",
            "│    └─bias_ih_l0                        [600]                                              ├─600\n",
            "│    └─bias_hh_l0                        [600]                                              └─600\n",
            "├─Linear: 1-3                            --               [32, 150]        [32, 289]        43,639\n",
            "│    └─weight                            [150, 289]                                         ├─43,350\n",
            "│    └─bias                              [289]                                              └─289\n",
            "========================================================================================================\n",
            "Total params: 223,739\n",
            "Trainable params: 223,739\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 297.46\n",
            "========================================================================================================\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 3.98\n",
            "Params size (MB): 0.89\n",
            "Estimated Total Size (MB): 4.89\n",
            "========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "model = SimpleLSTM(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model_stats = summary(model, input_data=batch_x, verbose=2, col_width=16,\n",
        "                      col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze The Model Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence shape\t  : torch.Size([32, 61])\n",
            "Embedding output shape\t  : torch.Size([32, 61, 100])\n",
            "\n",
            "LSTM internal states shape: torch.Size([32, 61, 150])\n",
            "LSTM hidden state shape\t  : torch.Size([1, 32, 150])\n",
            "LSTM cell state shape\t  : torch.Size([1, 32, 150])\n",
            "\n",
            "Fully Connected output shape: torch.Size([32, 289])\n"
          ]
        }
      ],
      "source": [
        "emb_layer_output = model.embedding(batch_x)\n",
        "print(\"Input sequence shape\\t  :\", batch_x.shape)\n",
        "print(\"Embedding output shape\\t  :\", emb_layer_output.shape)\n",
        "\n",
        "if hasattr(model, \"rnn\"):   \n",
        "    internal_states, hidden_state = model.rnn(emb_layer_output)\n",
        "    print(\"\\nRNN internal states shape:\", internal_states.shape)\n",
        "    print(\"RNN output state shape\\t :\", hidden_state.shape)\n",
        "if hasattr(model, \"lstm\"):\n",
        "    internal_states, (hidden_state, cell_state)= model.lstm(emb_layer_output)\n",
        "    print(\"\\nLSTM internal states shape:\", internal_states.shape)\n",
        "    print(\"LSTM hidden state shape\\t  :\", hidden_state.shape)\n",
        "    print(\"LSTM cell state shape\\t  :\", cell_state.shape)\n",
        "if hasattr(model, \"gru\"):   \n",
        "    internal_states, hidden_state = model.gru(emb_layer_output)\n",
        "    print(\"\\nGRU internal states shape:\", internal_states.shape)\n",
        "    print(\"GRU output state shape\\t :\", hidden_state.shape)\n",
        "\n",
        "fc_layer_output = model.fc(hidden_state.squeeze(0))\n",
        "print(\"\\nFully Connected output shape:\", fc_layer_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRLc1cbrYVVV",
        "outputId": "7bc238c9-56d3-4558-db07-41d2474aa64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/050 | epoch_loss: 166.90\n",
            "Epoch: 002/050 | epoch_loss: 145.94\n",
            "Epoch: 003/050 | epoch_loss: 132.18\n",
            "Epoch: 004/050 | epoch_loss: 119.31\n",
            "Epoch: 005/050 | epoch_loss: 106.40\n",
            "Epoch: 006/050 | epoch_loss: 94.58\n",
            "Epoch: 007/050 | epoch_loss: 83.77\n",
            "Epoch: 008/050 | epoch_loss: 74.32\n",
            "Epoch: 009/050 | epoch_loss: 65.67\n",
            "Epoch: 010/050 | epoch_loss: 57.34\n",
            "Epoch: 011/050 | epoch_loss: 49.97\n",
            "Epoch: 012/050 | epoch_loss: 43.83\n",
            "Epoch: 013/050 | epoch_loss: 38.60\n",
            "Epoch: 014/050 | epoch_loss: 33.15\n",
            "Epoch: 015/050 | epoch_loss: 29.26\n",
            "Epoch: 016/050 | epoch_loss: 25.77\n",
            "Epoch: 017/050 | epoch_loss: 22.66\n",
            "Epoch: 018/050 | epoch_loss: 20.11\n",
            "Epoch: 019/050 | epoch_loss: 17.73\n",
            "Epoch: 020/050 | epoch_loss: 15.91\n",
            "Epoch: 021/050 | epoch_loss: 14.47\n",
            "Epoch: 022/050 | epoch_loss: 13.12\n",
            "Epoch: 023/050 | epoch_loss: 11.95\n",
            "Epoch: 024/050 | epoch_loss: 11.29\n",
            "Epoch: 025/050 | epoch_loss: 10.11\n",
            "Epoch: 026/050 | epoch_loss: 9.32\n",
            "Epoch: 027/050 | epoch_loss: 8.79\n",
            "Epoch: 028/050 | epoch_loss: 8.34\n",
            "Epoch: 029/050 | epoch_loss: 7.77\n",
            "Epoch: 030/050 | epoch_loss: 7.39\n",
            "Epoch: 031/050 | epoch_loss: 7.06\n",
            "Epoch: 032/050 | epoch_loss: 6.78\n",
            "Epoch: 033/050 | epoch_loss: 6.51\n",
            "Epoch: 034/050 | epoch_loss: 6.17\n",
            "Epoch: 035/050 | epoch_loss: 6.08\n",
            "Epoch: 036/050 | epoch_loss: 5.77\n",
            "Epoch: 037/050 | epoch_loss: 5.60\n",
            "Epoch: 038/050 | epoch_loss: 5.53\n",
            "Epoch: 039/050 | epoch_loss: 5.29\n",
            "Epoch: 040/050 | epoch_loss: 5.12\n",
            "Epoch: 041/050 | epoch_loss: 4.98\n",
            "Epoch: 042/050 | epoch_loss: 4.85\n",
            "Epoch: 043/050 | epoch_loss: 4.78\n",
            "Epoch: 044/050 | epoch_loss: 4.80\n",
            "Epoch: 045/050 | epoch_loss: 4.65\n",
            "Epoch: 046/050 | epoch_loss: 4.46\n",
            "Epoch: 047/050 | epoch_loss: 4.48\n",
            "Epoch: 048/050 | epoch_loss: 4.35\n",
            "Epoch: 049/050 | epoch_loss: 4.34\n",
            "Epoch: 050/050 | epoch_loss: 4.28\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0\n",
        "  model = model.train()  \n",
        "  for batch_idx, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "    # forward pass\n",
        "    logits = model(batch_x)\n",
        "    loss = criterion(logits, batch_y)\n",
        "    \n",
        "    # backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    # if not batch_idx % 8:\n",
        "    #   print(f' -> batch {batch_idx+1:03d} | loss: {loss:.2f}')\n",
        "  \n",
        "  print(f'Epoch: {epoch+1:03d}/{epochs:03d} | epoch_loss: {epoch_loss:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O9f6DkX-ZM-r"
      },
      "outputs": [],
      "source": [
        "def predict(model, vocab, input_text):\n",
        "  tokenized_text = word_tokenize(input_text.lower())\n",
        "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "  padding_length = max_seq_length - 1 - len(numerical_text)\n",
        "  padded_text = torch.tensor([0]*padding_length + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "  logits = model(padded_text.to(device))\n",
        "  logit, index = torch.max(logits, dim=1)\n",
        "  return list(vocab.keys())[index], logit.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confidence: 10.83\n",
            "Answer: subscription\n"
          ]
        }
      ],
      "source": [
        "prediction, confidence = predict(model, vocab, \"The course follows a monthly\")\n",
        "print(f\"Confidence: {confidence:.2f}\")\n",
        "print(f\"Answer: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JPACfEbNPo",
        "outputId": "0a36a6da-8118-4989-b754-e69826765f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The course follows a monthly subscription\n",
            "The course follows a monthly subscription model\n",
            "The course follows a monthly subscription model where\n",
            "The course follows a monthly subscription model where you\n",
            "The course follows a monthly subscription model where you have\n",
            "The course follows a monthly subscription model where you have to\n",
            "The course follows a monthly subscription model where you have to make\n",
            "The course follows a monthly subscription model where you have to make monthly\n",
            "The course follows a monthly subscription model where you have to make monthly payments\n",
            "The course follows a monthly subscription model where you have to make monthly payments of\n",
            "The course follows a monthly subscription model where you have to make monthly payments of rs\n",
            "The course follows a monthly subscription model where you have to make monthly payments of rs 799/month\n"
          ]
        }
      ],
      "source": [
        "num_tokens = 12\n",
        "input_prompt = \"The course follows a monthly\"\n",
        "\n",
        "for _ in range(num_tokens):\n",
        "  prediction, logit = predict(model, vocab, input_prompt)  \n",
        "  input_prompt += \" \" + prediction\n",
        "  print(input_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py7o0rJJc5pm",
        "outputId": "d6de41b2-e157-4da6-8dde-cd27de358a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.96\n"
          ]
        }
      ],
      "source": [
        "def calculate_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            probs = model(batch_x)\n",
        "            _, predicted = torch.max(probs, dim=1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "test_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "accuracy = calculate_accuracy(model, test_dataloader)\n",
        "print(f\"Train Accuracy: {accuracy:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
